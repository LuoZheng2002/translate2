Loading configs from: config_slurm.py
Processing config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=NotTranslated(), add_noise_mode=<AddNoiseMode.NO_NOISE: 1>)
Warning: some test cases already exist in inference result file. Skipping 200 cases.
All test cases for Qwen/Qwen3-8B have already been processed. Skipping model loading and inference.
Creating vLLM pipeline for Qwen/Qwen3-8B
INFO 11-25 03:22:53 [__init__.py:239] Automatically detected platform cuda.
Loading local model with vLLM: Qwen/Qwen3-8B
INFO 11-25 03:23:18 [config.py:717] This model supports multiple tasks: {'generate', 'score', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.
INFO 11-25 03:23:18 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 11-25 03:23:21 [core.py:58] Initializing a V1 LLM engine (v0.8.5) with config: model='Qwen/Qwen3-8B', speculative_config=None, tokenizer='Qwen/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 11-25 03:23:23 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fbc1fa052e0>
INFO 11-25 03:23:24 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 11-25 03:23:24 [cuda.py:221] Using Flash Attention backend on V1 engine.
WARNING 11-25 03:23:24 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 11-25 03:23:24 [gpu_model_runner.py:1329] Starting to load model Qwen/Qwen3-8B...
INFO 11-25 03:23:25 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 11-25 03:23:36 [loader.py:458] Loading weights took 10.91 seconds
INFO 11-25 03:23:36 [gpu_model_runner.py:1347] Model loading took 15.2683 GiB and 12.282593 seconds
INFO 11-25 03:23:51 [backends.py:420] Using cache directory: /u/zluo8/.cache/vllm/torch_compile_cache/a8274931ea/rank_0_0 for vLLM's torch.compile
INFO 11-25 03:23:51 [backends.py:430] Dynamo bytecode transform time: 15.03 s
INFO 11-25 03:24:01 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 8.932 s
INFO 11-25 03:24:04 [monitor.py:33] torch.compile takes 15.03 s in total
INFO 11-25 03:24:05 [kv_cache_utils.py:634] GPU KV cache size: 132,560 tokens
INFO 11-25 03:24:05 [kv_cache_utils.py:637] Maximum concurrency for 4,096 tokens per request: 32.36x
INFO 11-25 03:24:29 [gpu_model_runner.py:1686] Graph capturing finished in 24 secs, took 0.60 GiB
INFO 11-25 03:24:29 [core.py:159] init engine (profile, create kv cache, warmup model) took 52.29 seconds
INFO 11-25 03:24:29 [core_client.py:439] Core engine process 0 ready.
WARNING 11-25 03:24:29 [sampling_params.py:347] temperature 0.001 is less than 0.01, which may cause numerical errors nan or inf in tensors. We have maxed it out to 0.01.
vLLM model loaded and ready for inference.
Post-processing: Copied 200 results without modification (DONT_POST_PROCESS)
Score result written to result/score/Qwen-Qwen3-8B/vanilla.json: {'accuracy': 0.95, 'total_cases': 200, 'correct_cases': 190}
Completed processing for config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=NotTranslated(), add_noise_mode=<AddNoiseMode.NO_NOISE: 1>)
Processing config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=NotTranslated(), add_noise_mode=<AddNoiseMode.SYNONYM: 2>)
Warning: some test cases already exist in inference result file. Skipping 200 cases.
All test cases for Qwen/Qwen3-8B have already been processed. Skipping model loading and inference.
Post-processing: Copied 200 results without modification (DONT_POST_PROCESS)
Score result written to result/score/Qwen-Qwen3-8B/syno.json: {'accuracy': 0.895, 'total_cases': 200, 'correct_cases': 179}
Completed processing for config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=NotTranslated(), add_noise_mode=<AddNoiseMode.SYNONYM: 2>)
Processing config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=NotTranslated(), add_noise_mode=<AddNoiseMode.PARAPHRASE: 3>)
Warning: some test cases already exist in inference result file. Skipping 200 cases.
All test cases for Qwen/Qwen3-8B have already been processed. Skipping model loading and inference.
Post-processing: Copied 200 results without modification (DONT_POST_PROCESS)
Score result written to result/score/Qwen-Qwen3-8B/para.json: {'accuracy': 0.94, 'total_cases': 200, 'correct_cases': 188}
Completed processing for config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=NotTranslated(), add_noise_mode=<AddNoiseMode.PARAPHRASE: 3>)
Processing config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=Translated(language=<Language.CHINESE: 1>, option=<TranslateOption.FULLY_TRANSLATED: 1>), add_noise_mode=<AddNoiseMode.NO_NOISE: 1>)
Warning: some test cases already exist in inference result file. Skipping 200 cases.
All test cases for Qwen/Qwen3-8B have already been processed. Skipping model loading and inference.
Post-processing: Copied 200 results without modification (DONT_POST_PROCESS)
Score result written to result/score/Qwen-Qwen3-8B/zh_f.json: {'accuracy': 0.51, 'total_cases': 200, 'correct_cases': 102}
Completed processing for config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=Translated(language=<Language.CHINESE: 1>, option=<TranslateOption.FULLY_TRANSLATED: 1>), add_noise_mode=<AddNoiseMode.NO_NOISE: 1>)
Processing config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=Translated(language=<Language.CHINESE: 1>, option=<TranslateOption.FULLY_TRANSLATED: 1>), add_noise_mode=<AddNoiseMode.SYNONYM: 2>)
Warning: some test cases already exist in inference result file. Skipping 200 cases.
All test cases for Qwen/Qwen3-8B have already been processed. Skipping model loading and inference.
Post-processing: Copied 200 results without modification (DONT_POST_PROCESS)
Score result written to result/score/Qwen-Qwen3-8B/zh_f_syno.json: {'accuracy': 0.495, 'total_cases': 200, 'correct_cases': 99}
Completed processing for config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=Translated(language=<Language.CHINESE: 1>, option=<TranslateOption.FULLY_TRANSLATED: 1>), add_noise_mode=<AddNoiseMode.SYNONYM: 2>)
Processing config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=Translated(language=<Language.CHINESE: 1>, option=<TranslateOption.FULLY_TRANSLATED: 1>), add_noise_mode=<AddNoiseMode.PARAPHRASE: 3>)
Warning: some test cases already exist in inference result file. Skipping 200 cases.
All test cases for Qwen/Qwen3-8B have already been processed. Skipping model loading and inference.
Post-processing: Copied 200 results without modification (DONT_POST_PROCESS)
Score result written to result/score/Qwen-Qwen3-8B/zh_f_para.json: {'accuracy': 0.52, 'total_cases': 200, 'correct_cases': 104}
Completed processing for config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=Translated(language=<Language.CHINESE: 1>, option=<TranslateOption.FULLY_TRANSLATED: 1>), add_noise_mode=<AddNoiseMode.PARAPHRASE: 3>)
Processing config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=Translated(language=<Language.CHINESE: 1>, option=<TranslateOption.FULLY_TRANSLATED_PROMPT_TRANSLATE: 2>), add_noise_mode=<AddNoiseMode.NO_NOISE: 1>)
Warning: some test cases already exist in inference result file. Skipping 200 cases.
All test cases for Qwen/Qwen3-8B have already been processed. Skipping model loading and inference.
Post-processing: Copied 200 results without modification (DONT_POST_PROCESS)
Score result written to result/score/Qwen-Qwen3-8B/zh_pt.json: {'accuracy': 0.59, 'total_cases': 200, 'correct_cases': 118}
Completed processing for config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=Translated(language=<Language.CHINESE: 1>, option=<TranslateOption.FULLY_TRANSLATED_PROMPT_TRANSLATE: 2>), add_noise_mode=<AddNoiseMode.NO_NOISE: 1>)
Processing config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=Translated(language=<Language.CHINESE: 1>, option=<TranslateOption.FULLY_TRANSLATED_PROMPT_TRANSLATE: 2>), add_noise_mode=<AddNoiseMode.SYNONYM: 2>)
Warning: some test cases already exist in inference result file. Skipping 200 cases.
All test cases for Qwen/Qwen3-8B have already been processed. Skipping model loading and inference.
Post-processing: Copied 200 results without modification (DONT_POST_PROCESS)
Score result written to result/score/Qwen-Qwen3-8B/zh_pt_syno.json: {'accuracy': 0.57, 'total_cases': 200, 'correct_cases': 114}
Completed processing for config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=Translated(language=<Language.CHINESE: 1>, option=<TranslateOption.FULLY_TRANSLATED_PROMPT_TRANSLATE: 2>), add_noise_mode=<AddNoiseMode.SYNONYM: 2>)
Processing config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=Translated(language=<Language.CHINESE: 1>, option=<TranslateOption.FULLY_TRANSLATED_PROMPT_TRANSLATE: 2>), add_noise_mode=<AddNoiseMode.PARAPHRASE: 3>)
Warning: some test cases already exist in inference result file. Skipping 200 cases.
All test cases for Qwen/Qwen3-8B have already been processed. Skipping model loading and inference.
Post-processing: Copied 200 results without modification (DONT_POST_PROCESS)
Score result written to result/score/Qwen-Qwen3-8B/zh_pt_para.json: {'accuracy': 0.62, 'total_cases': 200, 'correct_cases': 124}
Completed processing for config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=Translated(language=<Language.CHINESE: 1>, option=<TranslateOption.FULLY_TRANSLATED_PROMPT_TRANSLATE: 2>), add_noise_mode=<AddNoiseMode.PARAPHRASE: 3>)
Processing config: Config(model=<LocalModel.QWEN3_8B: 'Qwen/Qwen3-8B'>, translate_mode=Translated(language=<Language.CHINESE: 1>, option=<TranslateOption.FULLY_TRANSLATED_POST_PROCESS_DIFFERENT: 4>), add_noise_mode=<AddNoiseMode.NO_NOISE: 1>)
Warning: some test cases already exist in inference result file. Skipping 200 cases.
All test cases for Qwen/Qwen3-8B have already been processed. Skipping model loading and inference.
